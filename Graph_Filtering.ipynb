{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o1aoenA25n1",
        "outputId": "f808e13f-f6a5-426b-811e-34fbb9d4df33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NAEWyjL72xYY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats as st\n",
        "from scipy.special import expit\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from torch_geometric.nn import GCNConv, GraphConv, TAGConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import from_scipy_sparse_matrix\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from ipywidgets import interact, IntSlider\n",
        "from IPython import display\n",
        "\n",
        "import torch_geometric.utils\n",
        "from torch_geometric.datasets import Amazon, Planetoid, WikipediaNetwork, WebKB, Actor\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bAgb22CG_bvB"
      },
      "outputs": [],
      "source": [
        "def SGC(features, adjacency, order):\n",
        "  # retrieving the number of nodes from the adjacency matrix\n",
        "  num_nodes = adjacency.shape[0]\n",
        "\n",
        "  # adding self-loops to the adjacency matrix by adding an identity matrix of the same size\n",
        "  adjacency_augmented = adjacency + sp.sparse.eye(num_nodes)\n",
        "\n",
        "  # calculating the augmented degree of each node (degree + 1 due to self-loop)\n",
        "  degree_augmented = np.array(adjacency_augmented.sum(axis=1)).flatten()\n",
        "\n",
        "  # normalizing the augmented adjacency matrix\n",
        "  # multiplying the inverse square root of the degree matrix on both sides of the augmented adjacency matrix\n",
        "  adjacency_normalized = (\n",
        "      sp.sparse.spdiags(degree_augmented ** (-0.5), 0, num_nodes, num_nodes)\n",
        "      @ adjacency_augmented\n",
        "      @ sp.sparse.spdiags(degree_augmented ** (-0.5), 0, num_nodes, num_nodes)\n",
        "  )\n",
        "\n",
        "  # initializing the filtered_features variable with the input features\n",
        "  filtered_features = features\n",
        "\n",
        "  # propagating the features through the normalized adjacency matrix 'order' number of times\n",
        "  for _ in range(1, order + 1):\n",
        "    filtered_features = adjacency_normalized @ filtered_features\n",
        "\n",
        "  # returning the final filtered features after propagating through the graph\n",
        "  return filtered_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2LHbBO1Q8Q9b"
      },
      "outputs": [],
      "source": [
        "def ASGC(features, adjacency, order, regularization, return_coeffs=False, verbose=False):\n",
        "  # extracting the number of nodes and features from the input features matrix\n",
        "  num_nodes, num_features = features.shape\n",
        "\n",
        "  # computing the degree of each node in the adjacency matrix\n",
        "  degree = np.array(adjacency.sum(axis=1)).flatten()\n",
        "\n",
        "  # handling nodes with degree 0 (isolated nodes)\n",
        "  if 0 in degree:\n",
        "    if verbose:\n",
        "        print(\"Added self-loops where degree was 0.\")\n",
        "    # finding the indices of the nodes with degree 0\n",
        "    zero_degree_indices = np.argwhere(degree == 0).flatten()\n",
        "    # creating a vector to store the degree corrections\n",
        "    degree_fix = np.zeros(num_nodes)\n",
        "    degree_fix[zero_degree_indices] = 1\n",
        "    degree[zero_degree_indices] = 1.0\n",
        "    # updating the adjacency matrix to add self-loops to the isolated nodes\n",
        "    adjacency = adjacency + sp.sparse.spdiags(degree_fix, 0, num_nodes, num_nodes)\n",
        "\n",
        "  # normalizing the adjacency matrix\n",
        "  diag = sp.sparse.spdiags(degree ** (-0.5), 0, num_nodes, num_nodes)\n",
        "  adjacency_normalized = diag @ adjacency @ diag\n",
        "\n",
        "  # initializing an empty array to store the propagated features\n",
        "  feature_propagation = np.empty((order + 1, num_nodes, num_features))\n",
        "  feature_propagation[0, :, :] = features\n",
        "\n",
        "  # propagating the features through the normalized adjacency matrix\n",
        "  for i in range(1, order + 1):\n",
        "    feature_propagation[i, :, :] = adjacency_normalized @ feature_propagation[i - 1, :, :]\n",
        "\n",
        "  # initializing empty arrays for the coefficients and filtered features\n",
        "  coefficients = np.empty((num_features, order + 1))\n",
        "  filtered_features = np.empty((num_features, num_nodes))\n",
        "\n",
        "  # transposing the feature_propagation matrix for easier manipulation\n",
        "  feature_propagation = np.transpose(feature_propagation, (2, 1, 0))\n",
        "  \n",
        "  # creating the regularization vector\n",
        "  regularization_vector = np.zeros(order + 1)\n",
        "  regularization_vector[0] = np.sqrt(num_nodes * regularization)\n",
        "\n",
        "  # iterating through each feature\n",
        "  for feature_idx in range(num_features):\n",
        "    # stacking the feature propagation and regularization vector together\n",
        "    stacked_matrix = np.vstack((feature_propagation[feature_idx, :, :], regularization_vector[None, :]))\n",
        "\n",
        "    # creating a target vector with the first column of feature propagation and zeros at the end\n",
        "    target_vector = np.append(feature_propagation[feature_idx, :, 0], np.zeros(1))\n",
        "\n",
        "    # solving the least squares problem to find the coefficients for the current feature\n",
        "    coefficients[feature_idx, :], _, _, _ = np.linalg.lstsq(stacked_matrix, target_vector, rcond=None)\n",
        "\n",
        "    # applying the coefficients to the feature propagation matrix to get the filtered feature values\n",
        "    filtered_features[feature_idx, :] = feature_propagation[feature_idx, :, :] @ coefficients[feature_idx, :]\n",
        "\n",
        "    # printing progress if verbose is set to True\n",
        "    if verbose:\n",
        "        print(f\"Finished feature {feature_idx} of {num_features}.\")\n",
        "\n",
        "  # transposing the filtered_features matrix to match the input shape\n",
        "  filtered_features = filtered_features.T\n",
        "\n",
        "  # returning the filtered features and coefficients if requested, otherwise just the filtered features\n",
        "  if return_coeffs:\n",
        "    return filtered_features, coefficients\n",
        "  else:\n",
        "    return filtered_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-SZ7K0EU8Q9c"
      },
      "outputs": [],
      "source": [
        "def stochasticblock(n, communities, edge_probs, return_exp=False):\n",
        "  # initializing the expected adjacency matrix with zeros\n",
        "  adj_exp = np.zeros((n, n))\n",
        "\n",
        "  # determining the number of communities\n",
        "  num_communities = len(communities)\n",
        "\n",
        "  # iterating through the edge probabilities\n",
        "  for (i, j), prob in np.ndenumerate(edge_probs):\n",
        "      # updating the expected adjacency matrix using the edge probabilities\n",
        "      adj_exp[np.ix_(communities[i], communities[j])] = prob\n",
        "\n",
        "  # returning the expected adjacency matrix if requested\n",
        "  if return_exp:\n",
        "      return adj_exp\n",
        "  else:\n",
        "      # initializing the actual adjacency matrix with zeros\n",
        "      adj = np.zeros((n, n), dtype=np.int8)\n",
        "\n",
        "      # generating the actual adjacency matrix using the expected adjacency matrix\n",
        "      adj[np.triu_indices(n)] = np.random.rand(n * (n + 1) // 2) < adj_exp[np.triu_indices(n)]\n",
        "\n",
        "      # making the adjacency matrix symmetric\n",
        "      adj = adj + adj.T\n",
        "\n",
        "      # handling the diagonal elements (self-loops)\n",
        "      adj[np.diag_indices(n)] = adj[np.diag_indices(n)] // 2\n",
        "\n",
        "      # returning the actual adjacency matrix\n",
        "      return adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mRR6_pGT8Q9c"
      },
      "outputs": [],
      "source": [
        "def make_sbm_feat(n,p,q, noise_sigma=1., matshow=False):\n",
        "  # initializing the node labels\n",
        "  labels = np.zeros(n)\n",
        "  labels[n // 2:] = +1\n",
        "\n",
        "  # generating the adjacency matrix using the stochastic block model function\n",
        "  adj = stochasticblock(n, [np.arange(n // 2), np.arange(n // 2, n)], np.array([[p, q], [q, p]]))\n",
        "\n",
        "  # visualizing the adjacency matrix if requested\n",
        "  if matshow:\n",
        "    matfig = plt.figure(figsize=(8, 3), dpi=300)\n",
        "    plt.matshow(adj, fignum=matfig.number, aspect=1)\n",
        "\n",
        "  # creating the true features based on the labels\n",
        "  feats_true = 2. * labels - 1.\n",
        "\n",
        "  # adding noise to the true features to generate the observed features\n",
        "  feats = feats_true + np.random.normal(0, noise_sigma, size=n)\n",
        "\n",
        "  # converting the adjacency matrix to a sparse CSR matrix and returning it with the true and observed features\n",
        "  return sp.sparse.csr_matrix(adj), feats_true, feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRziHn1Z8Q9d"
      },
      "outputs": [],
      "source": [
        "n = 1000\n",
        "sum_exp = 10 * 10 / n\n",
        "p_q_logits = np.linspace(-2.5, 2.5, 3)\n",
        "\n",
        "#setting up the figure with 1 row and 4 columns of subplots\n",
        "fig, axs = plt.subplots(1, 4, figsize=(17 / 1.5, 5 / 1.5), dpi=100, tight_layout=True)\n",
        "titles = ['Heterophilous', 'Neither', 'Homophilous']\n",
        "\n",
        "#looping through each logit value to create corresponding plots\n",
        "for i, p_q_logit in enumerate(p_q_logits):\n",
        "    p_frac, q_frac = expit(p_q_logit), expit(-p_q_logit)\n",
        "    \n",
        "    #generating Stochastic Block Model (SBM) features with given probabilities\n",
        "    adj, feats_true, feats = make_sbm_feat(n=n, p=p_frac * sum_exp, q=q_frac * sum_exp, noise_sigma=0.5)\n",
        "    \n",
        "    #setting title and displaying adjacency matrix as heatmap\n",
        "    axs[i+1].set_title(titles[i])\n",
        "    axs[i+1].matshow(np.array(adj.todense()), cmap='Blues', vmin=0, vmax=1)\n",
        "    axs[i+1].set_xticks([0, int(n/2), n])\n",
        "    axs[i+1].set_yticks([0, int(n/2), n])\n",
        "    axs[i+1].xaxis.set_ticks_position(\"bottom\")\n",
        "\n",
        "#plotting features as scatter plot\n",
        "axs[0].scatter(np.arange(int(n/2)), feats[:int(n/2)], s=1, color='indianred')\n",
        "axs[0].scatter(np.arange(int(n/2), n), feats[int(n/2):n], s=1, color='steelblue')\n",
        "\n",
        "#adding horizontal lines at y = -1 and y = 1\n",
        "axs[0].plot([0, int(n/2)], [-1, -1], color='red')\n",
        "axs[0].plot([int(n/2), n], [+1, +1], color='blue')\n",
        "\n",
        "#setting grid, axis limits, and ticks for the scatter plot\n",
        "axs[0].grid()\n",
        "axs[0].set_ylim(-4, 4)\n",
        "axs[0].set_xlim(0, n)\n",
        "axs[0].set_xticks([0, int(n/2), n])\n",
        "\n",
        "#setting labels for the scatter plot\n",
        "axs[0].set_xlabel(\"Node\")\n",
        "axs[0].set_ylabel(\"Feature\")\n",
        "axs[0].set_aspect(abs(n - 0) / abs(4 - (-4)))\n",
        "axs[0].set_title(\"Features distributions\")\n",
        "\n",
        "#displaying the plots\n",
        "#plt.savefig(\"features.png\")\n",
        "#files.download(\"features.png\") \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXkDROy38Q9e"
      },
      "outputs": [],
      "source": [
        "#setting the number of nodes in the graph\n",
        "num_nodes = 1000\n",
        "#calculating the sum of probabilities p and q\n",
        "pq_sum = 10 / num_nodes\n",
        "#setting the lambda2 constant\n",
        "lambda2 = -0.9\n",
        "#calculating the individual probabilities p and q (inner and outer connections probabilities)\n",
        "p = (pq_sum + lambda2 * pq_sum) / 2\n",
        "q = (pq_sum - lambda2 * pq_sum) / 2\n",
        "#generating the features using the given probabilities\n",
        "adjacency_matrix, true_features, generated_features = make_sbm_feat(num_nodes, p, q, 1.0)\n",
        "#computing the sum of the adjacency matrix along the given axis\n",
        "degree = np.array(adjacency_matrix.sum(axis=1)).flatten()\n",
        "#creating a copy of the degree array\n",
        "degree_no_zero = degree.copy()\n",
        "#replacing zero degrees with one\n",
        "degree_no_zero[degree == 0] = 1\n",
        "#defining minimum and maximum iterations for the precomputation (values of K, filterig steps)\n",
        "min_iterations = 1\n",
        "max_iterations = 10\n",
        "\n",
        "precomputed_histograms = {}\n",
        "\n",
        "#looping through the range of iterations\n",
        "for iterations in range(min_iterations, max_iterations + 1):\n",
        "    #applying Simple Graph Convolution (SGC) to the generated features\n",
        "    features_SGC = SGC(generated_features, adjacency_matrix, iterations)\n",
        "    #applying Adaptive Simple Graph Convolution (ASGC) to the generated features\n",
        "    features_ASGC, coefficients = ASGC(generated_features.reshape(num_nodes, 1), adjacency_matrix, iterations, 1e10, True, False)\n",
        "    features_ASGC = features_ASGC.flatten()\n",
        "    \n",
        "    #storing the generated features, SGC features, and ASGC features in a list\n",
        "    data = [generated_features, features_SGC, features_ASGC]\n",
        "    #adding the list of features to the precomputed_histograms dictionary\n",
        "    precomputed_histograms[iterations] = data\n",
        "\n",
        "#defining the plot_histograms function that takes 'iterations' as input\n",
        "def plot_histograms(iterations):\n",
        "    #creating a figure with 3 subplots\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(5, 6), dpi=80, tight_layout=True, sharex=True, sharey=True)\n",
        "\n",
        "    #defining colors and titles for the histograms\n",
        "    colors = {'raw': 'indianred', 'sgc': 'steelblue', 'asgc': 'darkorange'}\n",
        "    titles = ['Raw', 'SGC', 'ASGC']\n",
        "    #retrieving the precomputed data for the given number of iterations\n",
        "    data = precomputed_histograms[iterations]\n",
        "\n",
        "    #setting the opacity (alpha) and bin size for the histograms\n",
        "    alpha = 0.9\n",
        "    bins = np.linspace(-4, 4, 41)\n",
        "\n",
        "    #looping through the subplots and plotting the histograms\n",
        "    for idx, ax in enumerate(axes):\n",
        "        #plotting the histograms for the first half of the dataset (red) and the second half (blue)\n",
        "        ax.hist(data[idx][:int(num_nodes / 2)], alpha=alpha, bins=bins, color=colors['raw'])\n",
        "        ax.hist(data[idx][int(num_nodes / 2):], alpha=alpha, bins=bins, color=colors['sgc'])\n",
        "        #adding vertical dashed lines for red and blue means\n",
        "        ax.axvline(-1, color='red', linestyle='--')\n",
        "        ax.axvline(+1, color='blue', linestyle='--')\n",
        "        \n",
        "        #calculating and displaying mean and standard deviation for\n",
        "        red_mean = np.mean(data[idx][:int(num_nodes / 2)])\n",
        "        blue_mean = np.mean(data[idx][int(num_nodes / 2):])\n",
        "        red_std = np.std(data[idx][:int(num_nodes / 2)])\n",
        "        blue_std = np.std(data[idx][int(num_nodes / 2):])\n",
        "        title = f\"{titles[idx]} \\n (R: μ={red_mean:.2f}, σ={red_std:.2f} | B: μ={blue_mean:.2f}, σ={blue_std:.2f})\"\n",
        "        ax.set_title(title)\n",
        "\n",
        "        ax.set_xlim(-4, 4)\n",
        "        ax.set_ylim(0, 150)\n",
        "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "                     ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "            item.set_fontsize(item.get_fontsize() * 1.2)\n",
        "\n",
        "    #lt.show()\n",
        "\n",
        "interact(plot_histograms, iterations=IntSlider(min=min_iterations, max=max_iterations, step=1, value=0, continuous_update=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yojks8NOpFt"
      },
      "outputs": [],
      "source": [
        "#setting the number of nodes in the graph\n",
        "num_nodes = 1000\n",
        "#calculating the sum of probabilities p and q\n",
        "pq_sum = 10 / num_nodes\n",
        "#setting the lambda2 constant\n",
        "lambda2 = -0.9\n",
        "#calculating the individual probabilities p and q (inner and outer connections probabilities)\n",
        "p = (pq_sum + lambda2 * pq_sum) / 2\n",
        "q = (pq_sum - lambda2 * pq_sum) / 2\n",
        "#generating the features using the given probabilities\n",
        "adjacency_matrix, true_features, generated_features = make_sbm_feat(num_nodes, p, q, 1.0)\n",
        "#computing the sum of the adjacency matrix along the given axis\n",
        "degree = np.array(adjacency_matrix.sum(axis=1)).flatten()\n",
        "#creating a copy of the degree array\n",
        "degree_no_zero = degree.copy()\n",
        "#replacing zero degrees with one\n",
        "degree_no_zero[degree == 0] = 1\n",
        "#defining minimum and maximum iterations for the precomputation (values of K, filterig steps)\n",
        "min_iterations = 1\n",
        "max_iterations = 10\n",
        "alpha = 0.9\n",
        "bins = np.linspace(-4,4,41)\n",
        "\n",
        "fig, axs = plt.subplots(3,4, figsize=(8,6), dpi=100, tight_layout=True, sharex=True, sharey=False)\n",
        "\n",
        "feats_list_sgc = []\n",
        "feats_list_asgc = []\n",
        "\n",
        "for i, K in enumerate([1,2,4,8]):\n",
        "    feats_list_sgc.append(SGC(generated_features, adjacency_matrix, K))\n",
        "    features_ASGC, coefficients = ASGC(generated_features.reshape(num_nodes, 1), adjacency_matrix, K, 1e10, True, False)\n",
        "    feats_list_asgc.append(features_ASGC.flatten())\n",
        "\n",
        "    axs[0,i].hist(generated_features[:int(num_nodes/2)], alpha=alpha, bins=bins, color='indianred')\n",
        "    axs[0,i].hist(generated_features[int(num_nodes/2):], alpha=alpha, bins=bins, color='steelblue')\n",
        "    axs[0,i].axvline(-1, color='red', linestyle='--')\n",
        "    axs[0,i].axvline(+1, color='blue', linestyle='--')\n",
        "    axs[0,i].set_title(f\"Raw, K={K}\")\n",
        "    axs[0,i].set_ylim(0,150)\n",
        "\n",
        "    axs[1,i].hist(feats_list_sgc[i][:int(num_nodes/2)], alpha=alpha, bins=bins, color='indianred')\n",
        "    axs[1,i].hist(feats_list_sgc[i][int(num_nodes/2):], alpha=alpha, bins=bins, color='steelblue')\n",
        "    axs[1,i].axvline(-1, color='red', linestyle='--')\n",
        "    axs[1,i].axvline(+1, color='blue', linestyle='--')\n",
        "    axs[1,i].set_title(f\"SGC, K={K}\")\n",
        "    axs[1,i].set_ylim(0,150)  # setting y limit to 200 for the second row\n",
        "\n",
        "    axs[2,i].hist(feats_list_asgc[i][:int(num_nodes/2)], alpha=alpha, bins=bins, color='indianred')\n",
        "    axs[2,i].hist(feats_list_asgc[i][int(num_nodes/2):], alpha=alpha, bins=bins, color='steelblue')\n",
        "    axs[2,i].axvline(-1, color='red', linestyle='--')\n",
        "    axs[2,i].axvline(+1, color='blue', linestyle='--')\n",
        "    axs[2,i].set_title(f\"ASGC, K={K}\")\n",
        "    axs[2,i].set_ylim(0,150)\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.set_xlim(-4,4)\n",
        "\n",
        "#plt.savefig(\"ks8.png\")\n",
        "#files.download(\"ks8.png\") \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wiRxA5cb8Q9e"
      },
      "outputs": [],
      "source": [
        "# Define the parameters\n",
        "num_nodes = 1000\n",
        "pq_sum = 10 / num_nodes\n",
        "lambda2_vals = np.linspace(-1, 1, 51)\n",
        "num_trials = 100\n",
        "num_configs = len(lambda2_vals)\n",
        "K = 2\n",
        "\n",
        "# Define the arrays to store the results\n",
        "sgc_mae = np.empty((num_configs, num_trials))\n",
        "asgc_mae = np.empty((num_configs, num_trials))\n",
        "raw_mae = np.empty((num_configs, num_trials))\n",
        "sgc_sign = np.empty((num_configs, num_trials))\n",
        "asgc_sign = np.empty((num_configs, num_trials))\n",
        "raw_sign = np.empty((num_configs, num_trials))\n",
        "\n",
        "# Generate the SBM graphs and calculate the SGC and ASGC scores for each graph\n",
        "for i, lambda2 in enumerate(lambda2_vals):\n",
        "    p = (pq_sum + lambda2 * pq_sum) / 2\n",
        "    q = (pq_sum - lambda2 * pq_sum) / 2\n",
        "    for trial in range(num_trials):\n",
        "        adj_matrix, true_feats, noisy_feats = make_sbm_feat(\n",
        "            n=num_nodes, p=p, q=q, noise_sigma=1.0)\n",
        "        sgc_feats = SGC(noisy_feats, adj_matrix, K)\n",
        "        asgc_feats = ASGC(noisy_feats.reshape(num_nodes, 1), adj_matrix, K, 1e10).flatten()\n",
        "        sgc_mae[i, trial] = np.mean(np.abs(sgc_feats - true_feats))\n",
        "        asgc_mae[i, trial] = np.mean(np.abs(asgc_feats - true_feats))\n",
        "        raw_mae[i, trial] = np.mean(np.abs(noisy_feats - true_feats))\n",
        "        sgc_sign[i, trial] = np.mean(np.sign(sgc_feats) == np.sign(true_feats))\n",
        "        asgc_sign[i, trial] = np.mean(np.sign(asgc_feats) == np.sign(true_feats))\n",
        "        raw_sign[i, trial] = np.mean(np.sign(noisy_feats) == np.sign(true_feats))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4lBKiEhJzmm"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(5, 4), dpi=100, sharex=True, tight_layout=True)\n",
        "\n",
        "#plotting the mean absolute errors for each method\n",
        "axs[0].plot(lambda2_vals, np.ones(num_configs) * np.mean(raw_mae), label='Raw', linestyle='--', color='black', alpha=0.9)\n",
        "sgc_line = axs[0].plot(lambda2_vals, np.mean(sgc_mae, axis=1), label='SGC', color='red', alpha=0.9)\n",
        "asgc_line = axs[0].plot(lambda2_vals, np.mean(asgc_mae, axis=1), label='ASGC', color='blue', alpha=0.9)\n",
        "axs[0].set_ylabel(\"Mean Absolute Error\")\n",
        "axs[0].grid()\n",
        "\n",
        "#filling the area between the ASGC and SGC curves\n",
        "sgc_y = np.mean(sgc_mae, axis=1)\n",
        "asgc_y = np.mean(asgc_mae, axis=1)\n",
        "axs[0].fill_between(lambda2_vals, sgc_y, asgc_y, where=asgc_y <= sgc_y, interpolate=True, color='blue', alpha=.25, label='Gain')\n",
        "axs[0].fill_between(lambda2_vals, sgc_y, asgc_y, where=asgc_y > sgc_y, interpolate=True, color='red', alpha=.25, label='Loss')\n",
        "\n",
        "#setting the x-axis labels for the second plot\n",
        "axs[1].set_xlabel(r'$\\frac{p-q}{p+q}$', labelpad=-5)\n",
        "axs[1].set_xticks([-1.0, -0.5, 0, +0.5, +1])\n",
        "axs[1].set_xticklabels(['-1.0\\n(Heterophilous)', '-0.5', '0', '+0.5', '+1.0\\n(Homophilous)'])\n",
        "axs[1].grid()\n",
        "\n",
        "#plotting the sign errors for each method\n",
        "axs[1].plot(lambda2_vals, 1.-np.ones(num_configs)*raw_sign.mean(), label='Raw', linestyle='--', color='black', alpha=0.9)\n",
        "axs[1].plot(lambda2_vals, 1. - np.mean(sgc_sign, axis=1), label='SGC', color='red', alpha=0.9)\n",
        "axs[1].plot(lambda2_vals, 1. - np.mean(asgc_sign, axis=1), label='ASGC', color='blue', alpha=0.9)\n",
        "axs[1].set_ylabel(\"Sign Error\")\n",
        "axs[1].grid()\n",
        "\n",
        "#filling the area between the ASGC and SGC curves\n",
        "sgc_y = 1. - np.mean(sgc_sign, axis=1)\n",
        "asgc_y = 1. - np.mean(asgc_sign, axis=1)\n",
        "axs[1].fill_between(lambda2_vals, sgc_y, asgc_y, where=asgc_y <= sgc_y, interpolate=True, color='blue', alpha=.25, label='Gain')\n",
        "axs[1].fill_between(lambda2_vals, sgc_y, asgc_y, where=asgc_y > sgc_y, interpolate=True, color='red', alpha=.25, label='Loss')\n",
        "axs[1].grid()\n",
        "\n",
        "#adding the legend and adjust the plot layout\n",
        "handles, labels = axs[1].get_legend_handles_labels()\n",
        "lgd = fig.legend(handles, labels, bbox_to_anchor=[1.125, 0.65])\n",
        "plt.tight_layout()\n",
        "#plt.savefig(\"errors.png\", bbox_inches='tight')\n",
        "#files.download(\"errors.png\") \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkrqoJXWBcd8",
        "outputId": "a5019b35-52a6-4879-8280-89fd59005b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "def load_dataset(dataset_name):\n",
        "    # loading Amazon datasets\n",
        "    if dataset_name in ['computers', 'photo']:\n",
        "        data = Amazon('./', dataset_name).data\n",
        "    # loading Planetoid datasets\n",
        "    elif dataset_name in ['cora', 'citeseer', 'pubmed']:\n",
        "        data = Planetoid('./', dataset_name).data\n",
        "    # loading WikipediaNetwork datasets\n",
        "    elif dataset_name in ['chameleon', 'squirrel']:\n",
        "        data = WikipediaNetwork('./', dataset_name).data\n",
        "    # loading Actor dataset\n",
        "    elif dataset_name in ['film']:\n",
        "        data = Actor('./', dataset_name).data\n",
        "    # loading WebKB datasets\n",
        "    elif dataset_name in ['cornell', 'texas']:\n",
        "        data = WebKB('./', dataset_name).data\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    # converting edge_index to scipy sparse matrix\n",
        "    adj = torch_geometric.utils.to_scipy_sparse_matrix(data.edge_index)\n",
        "    # extracting features and labels\n",
        "    feats = np.array(data.node_stores[0]['x'])\n",
        "    labels = np.array(data.node_stores[0]['y'])\n",
        "    # making sure adjacency matrix is symmetric\n",
        "    adj = adj.maximum(adj.T)\n",
        "\n",
        "    return adj, feats, labels\n",
        "\n",
        "\n",
        "# defining names of homophilic datasets\n",
        "homoph_dsets = ['cora', 'citeseer', 'pubmed', 'computers', 'photo']\n",
        "# defining names of heterophilic datasets\n",
        "heteroph_dsets = ['chameleon', 'squirrel', 'film', 'texas', 'cornell']\n",
        "all_dsets = homoph_dsets + heteroph_dsets\n",
        "\n",
        "# predownloading the datasets\n",
        "for d in all_dsets:\n",
        "    _, _, _ = load_dataset(d)\n",
        "\n",
        "# defining a function to calculate the gap between the mean and upper bound of the 95% confidence interval\n",
        "conf_int_95 = lambda a: st.t.interval(0.95, len(a) - 1, loc=np.mean(a), scale=st.sem(a))[1] - np.mean(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jBQkvT-5A-Er"
      },
      "outputs": [],
      "source": [
        "# defining a function to load and process a dataset\n",
        "def process_dataset(dset):\n",
        "    # loading dataset\n",
        "    adj, feats, labels = load_dataset(dset)\n",
        "    # calculating the number of nodes\n",
        "    n = adj.shape[0]\n",
        "    # calculating the number of features and labels\n",
        "    num_features, num_labels = feats.shape[1], len(set(labels))\n",
        "    # printing out the dataset details\n",
        "    print(\n",
        "        f\"Dataset '{dset}' loaded. There are {n} nodes, {(adj[np.triu_indices(n)] > 0).sum()} edges, {num_features} features, and {num_labels} labels.\")\n",
        "    # setting proportions for test and validation sets depending on the type of dataset\n",
        "    test_prop, valid_prop = (0.95, 0.5) if dset in homoph_dsets else (0.2, 0.25)\n",
        "    # returning processed data\n",
        "    return adj, feats, labels, n, test_prop, valid_prop\n",
        "\n",
        "\n",
        "# defining a function to calculate accuracy with given features\n",
        "def get_acc_with_feats(filtered_feats, random_state, valid, nontest_idcs, test_idcs, labels):\n",
        "    # splitting data into training and validation sets if 'valid' is True\n",
        "    if valid:\n",
        "        train_idcs, valid_idcs = train_test_split(nontest_idcs, test_size=valid_prop, random_state=random_state)\n",
        "        # fitting logistic regression model and calculating accuracy for validation set\n",
        "        clf = LogisticRegression(max_iter=logreg_maxiter).fit(filtered_feats[train_idcs, :], labels[train_idcs])\n",
        "        return (labels[valid_idcs] == clf.predict(filtered_feats[valid_idcs, :])).mean()\n",
        "    else:\n",
        "        # fitting logistic regression model and calculating accuracy for test set if 'valid' is False\n",
        "        clf = LogisticRegression(max_iter=logreg_maxiter).fit(filtered_feats[nontest_idcs, :], labels[nontest_idcs])\n",
        "        return (labels[test_idcs] == clf.predict(filtered_feats[test_idcs, :])).mean()\n",
        "\n",
        "\n",
        "# using raw features\n",
        "def calculate_raw_test_acc(dset, feats, random_states, n, test_prop, labels):\n",
        "    # initializing an empty array to store raw test accuracies\n",
        "    raw_test_accs[dset] = np.empty((num_trials))\n",
        "    raw_tt_time[dset] = np.empty((num_trials))\n",
        "    # iterating over each random state\n",
        "    for i, random_state in enumerate(random_states):\n",
        "        # splitting indices into non-test and test sets\n",
        "        t = time.time()\n",
        "        nontest_idcs, test_idcs = train_test_split(np.arange(n), test_size=test_prop, random_state=random_state)\n",
        "        # computing and storing the test accuracy for each trial using raw features\n",
        "        raw_test_accs[dset][i] = get_acc_with_feats(feats, random_state, valid=False, nontest_idcs=nontest_idcs,\n",
        "                                                    test_idcs=test_idcs, labels=labels)\n",
        "        raw_tt_time[dset][i] = time.time() - t\n",
        "    # printing the mean test accuracy and the 95% confidence interval\n",
        "    print(\n",
        "        f\"\\tTest accuracy with raw is {100 * raw_test_accs[dset].mean():.2f} +/- {100 * conf_int_95(raw_test_accs[dset]):.2f}\")\n",
        "\n",
        "\n",
        "# defining a function to calculate test accuracy using simple graph convolution (SGC)\n",
        "def calculate_sgc_test_acc(dset, feats, adj, random_states, n, test_prop, valid_prop, labels):\n",
        "    # initializing empty arrays to store validation accuracies, best K indices and test accuracies for SGC\n",
        "    sgc_valid_accs[dset] = np.empty((num_trials, len(K_vals)))\n",
        "    sgc_best_K_idcs[dset] = np.empty((num_trials), dtype=int)\n",
        "    sgc_test_accs[dset] = np.empty((num_trials))\n",
        "    sgc_tt_time[dset] = np.empty((num_trials))\n",
        "    # iterating over each random state\n",
        "    for i, random_state in enumerate(random_states):\n",
        "        # splitting indices into non-test and test sets\n",
        "        nontest_idcs, test_idcs = train_test_split(np.arange(n), test_size=test_prop, random_state=random_state)\n",
        "        # iterating over each possible K value\n",
        "        for K_num, K_val in enumerate(K_vals):\n",
        "            # applying SGC to the features\n",
        "            feats_filtered_sgc = SGC(feats, adj, K_val)\n",
        "            # computing and storing the validation accuracy for each K\n",
        "            sgc_valid_accs[dset][i, K_num] = get_acc_with_feats(feats_filtered_sgc, random_state, valid=True,\n",
        "                                                                nontest_idcs=nontest_idcs, test_idcs=test_idcs,\n",
        "                                                                labels=labels)\n",
        "            # finding the K that gives the highest validation accuracy\n",
        "        sgc_best_K_idcs[dset][i] = sgc_valid_accs[dset][i].argmax()\n",
        "        t = time.time()\n",
        "        # applying SGC to the features with the best K\n",
        "        feats_filtered_sgc = SGC(feats, adj, K_vals[sgc_best_K_idcs[dset][i]])\n",
        "        # computing and storing the test accuracy using the best K\n",
        "        sgc_test_accs[dset][i] = get_acc_with_feats(feats_filtered_sgc, random_state, valid=False,\n",
        "                                                    nontest_idcs=nontest_idcs, test_idcs=test_idcs, labels=labels)\n",
        "        sgc_tt_time[dset][i] = time.time() - t\n",
        "\n",
        "    # printing the mean test accuracy and the 95% confidence interval\n",
        "    print(\n",
        "        f\"\\tTest accuracy with SGC is {100 * sgc_test_accs[dset].mean():.2f} +/- {100 * conf_int_95(sgc_test_accs[dset]):.2f}\")\n",
        "\n",
        "\n",
        "def calculate_asgc_test_acc(dset, feats, adj, random_states, n, test_prop, valid_prop, labels):\n",
        "    # initializing empty arrays to store validation accuracies, best K indices, best Rp indices, and test accuracies for ASGC\n",
        "    asgc_valid_accs[dset] = np.empty((num_trials, len(K_vals), len(Rp_vals)))\n",
        "    asgc_best_K_idcs[dset] = np.empty((num_trials), dtype=int)\n",
        "    asgc_best_Rp_idcs[dset] = np.empty((num_trials), dtype=int)\n",
        "    asgc_test_accs[dset] = np.empty((num_trials))\n",
        "    asgc_tt_time[dset] = np.empty((num_trials))\n",
        "    # iterating over each random state\n",
        "    for i, random_state in enumerate(random_states):\n",
        "        # splitting indices into non-test and test sets\n",
        "        nontest_idcs, test_idcs = train_test_split(np.arange(n), test_size=test_prop, random_state=random_state)\n",
        "        # iterating over each possible K value and regularization parameter Rp\n",
        "        for K_num, K_val in enumerate(K_vals):\n",
        "            for Rp_num, Rp_val in enumerate(Rp_vals):\n",
        "                # applying ASGC to the features\n",
        "                feats_filtered_asgc = ASGC(feats, adj, K_val, Rp_val)\n",
        "                # computing and storing the validation accuracy for each K and Rp\n",
        "                asgc_valid_accs[dset][i, K_num, Rp_num] = get_acc_with_feats(feats_filtered_asgc, random_state,\n",
        "                                                                             valid=True, nontest_idcs=nontest_idcs,\n",
        "                                                                             test_idcs=test_idcs, labels=labels)\n",
        "        # finding the K and Rp that give the highest validation accuracy\n",
        "        asgc_best_K_idcs[dset][i], asgc_best_Rp_idcs[dset][i] = np.unravel_index(asgc_valid_accs[dset][i].argmax(),\n",
        "                                                                                 (len(K_vals), len(Rp_vals)))\n",
        "        t = time.time()\n",
        "        # applying ASGC to the features with the best K and Rp\n",
        "        feats_filtered_asgc = ASGC(feats, adj, K_vals[asgc_best_K_idcs[dset][i]], Rp_vals[asgc_best_Rp_idcs[dset][i]])\n",
        "        # computing and storing the test accuracy using the best K and Rp\n",
        "        asgc_test_accs[dset][i] = get_acc_with_feats(feats_filtered_asgc, random_state, valid=False,\n",
        "                                                     nontest_idcs=nontest_idcs, test_idcs=test_idcs, labels=labels)\n",
        "        asgc_tt_time[dset][i] = time.time() - t\n",
        "\n",
        "    # printing the mean test accuracy and the 95% confidence interval\n",
        "    print(\n",
        "        f\"\\tTest accuracy with ASGC is {100 * asgc_test_accs[dset].mean():.2f} +/- {100 * conf_int_95(asgc_test_accs[dset]):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tY1d3MBEBYYg"
      },
      "outputs": [],
      "source": [
        "def create_mlp_model(input_dim, output_dim):\n",
        "    # creating the MLP model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(4000, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(2000, activation='relu'),\n",
        "        tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def calculate_mlp_test_acc(dset, feats, random_states, n, test_prop, valid_prop, labels):\n",
        "    # Initializing an empty array to store MLP test accuracies\n",
        "    mlp_test_accs[dset] = np.empty((len(random_states)))\n",
        "    mlp_tt_time[dset] = np.empty((num_trials))\n",
        "    # Transforming labels into one-hot vectors\n",
        "    lb = LabelBinarizer()\n",
        "    labels = lb.fit_transform(labels)\n",
        "    # Iterating over each random state\n",
        "    for i, random_state in enumerate(random_states):\n",
        "        t = time.time()\n",
        "        # Splitting indices into non-test and test sets\n",
        "        nontest_idcs, test_idcs = train_test_split(np.arange(n), test_size=test_prop, random_state=random_state)\n",
        "        # Creating the MLP model\n",
        "        model = create_mlp_model(feats.shape[1], labels.shape[1])\n",
        "        # Training the model with early stopping\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        model.fit(feats[nontest_idcs, :], labels[nontest_idcs], callbacks=[early_stopping], validation_split=valid_prop,\n",
        "                  epochs=50, batch_size=32, verbose=0)\n",
        "        # Evaluating the model and storing the test accuracy\n",
        "        _, accuracy = model.evaluate(feats[test_idcs, :], labels[test_idcs], verbose=0)\n",
        "        mlp_test_accs[dset][i] = accuracy\n",
        "        mlp_tt_time[dset][i] = time.time() - t\n",
        "    # Printing the mean test accuracy\n",
        "    print(\n",
        "        f\"\\tTest accuracy with MLP is {100 * mlp_test_accs[dset].mean():.2f} +/- {100 * conf_int_95(mlp_test_accs[dset]):.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Yki539EsBV8h"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 2000)\n",
        "        # self.conv1 = TAGConv(num_features, num_features*2, K=2)\n",
        "        self.conv2 = GCNConv(2000, num_classes)\n",
        "        # self.conv2 = TAGConv(num_features*2, num_classes, K=2)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def train_and_test(dataset_name, seed, adj, feats, labels):\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # according to the paper\n",
        "    if dataset_name in homoph_dsets:\n",
        "        test_prop = 0.95\n",
        "        valid_prop = 0.025\n",
        "    else:\n",
        "        test_prop = 0.2\n",
        "        valid_prop = 0.2\n",
        "\n",
        "    edge_index = from_scipy_sparse_matrix(adj)[0]\n",
        "    x = torch.from_numpy(feats).float()\n",
        "    y = torch.from_numpy(labels).long()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net(x.size(1), len(np.unique(y))).to(device)\n",
        "    x = x.to(device)\n",
        "    edge_index = edge_index.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "    # calculate the indices for train, validation and test data\n",
        "    train_index = torch.arange(0, int((1 - test_prop - valid_prop) * y.size(0)), dtype=torch.long).to(device)\n",
        "    val_index = torch.arange(int((1 - test_prop - valid_prop) * y.size(0)), int((1 - test_prop) * y.size(0)),\n",
        "                             dtype=torch.long).to(device)\n",
        "    test_index = torch.arange(int((1 - test_prop) * y.size(0)), y.size(0), dtype=torch.long).to(device)\n",
        "\n",
        "    highest_val = 0\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x, edge_index)\n",
        "        loss = F.nll_loss(out[train_index], y[train_index])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        _, pred = model(x, edge_index).max(dim=1)\n",
        "        correct = float(pred[val_index].eq(y[val_index]).sum().item())\n",
        "        val_acc = correct / val_index.size(0)\n",
        "\n",
        "        if highest_val < val_acc:\n",
        "            highest_val = val_acc\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "\n",
        "        if patience > 9:\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    _, pred = model(x, edge_index).max(dim=1)\n",
        "    correct = float(pred[test_index].eq(y[test_index]).sum().item())\n",
        "    acc = correct / test_index.size(0)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def calculate_gnn_test_acc(dset, random_states, adj, feats, labels):\n",
        "    gcn_test_accs[dset] = np.empty((len(random_states)))\n",
        "    gcn_tt_time[dset] = np.empty((num_trials))\n",
        "    # Iterating over each random state\n",
        "    for i, random_state in enumerate(random_states):\n",
        "        t = time.time()\n",
        "        gcn_test_accs[dset][i] = train_and_test(dset, random_state, adj, feats, labels)\n",
        "        gcn_tt_time[dset][i] = time.time() - t\n",
        "\n",
        "    # Printing the mean test accuracy\n",
        "    print(f\"\\tTest accuracy with GCN is {100 * gcn_test_accs[dset].mean():.2f} +/- {100 * conf_int_95(gcn_test_accs[dset]):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk-VGlUmBSvU"
      },
      "outputs": [],
      "source": [
        "# initializing dictionaries to store results of validations and tests\n",
        "sgc_valid_accs, asgc_valid_accs = {}, {}\n",
        "sgc_best_K_idcs, asgc_best_K_idcs, asgc_best_Rp_idcs = {}, {}, {}\n",
        "raw_test_accs, sgc_test_accs, asgc_test_accs, mlp_test_accs, gcn_test_accs = {}, {}, {}, {}, {}\n",
        "raw_tt_time, sgc_tt_time, asgc_tt_time, mlp_tt_time, gcn_tt_time = {}, {}, {}, {}, {}\n",
        "\n",
        "# setting the maximum number of iterations for logistic regression\n",
        "logreg_maxiter = 1000\n",
        "num_trials = 10\n",
        "# defining possible values for number of hops (K)\n",
        "K_vals = np.array([1, 2, 3, 4, 6, 8])\n",
        "# defining possible values for the zero coefficient regularization (Rp)\n",
        "Rp_vals = np.geomspace(1e-6, 1e6, num=13)\n",
        "# setting random states for the reproducibility of the experiment\n",
        "random_states = 42 * np.arange(num_trials)\n",
        "\n",
        "# Main loop for processing datasets\n",
        "for dset in all_dsets:\n",
        "    adj, feats, labels, n, test_prop, valid_prop = process_dataset(dset)\n",
        "\n",
        "    calculate_raw_test_acc(dset, feats, random_states, n, test_prop, labels)\n",
        "    calculate_sgc_test_acc(dset, feats, adj, random_states, n, test_prop, valid_prop, labels)\n",
        "    calculate_asgc_test_acc(dset, feats, adj, random_states, n, test_prop, valid_prop, labels)\n",
        "    calculate_mlp_test_acc(dset, feats, random_states, n, test_prop, valid_prop, labels)\n",
        "    calculate_gnn_test_acc(dset, random_states, adj, feats, labels)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoXzq-1WBPsg"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_raw = pd.DataFrame(raw_test_accs)\n",
        "df_sgc = pd.DataFrame(sgc_test_accs)\n",
        "df_asgc = pd.DataFrame(asgc_test_accs)\n",
        "df_mlp = pd.DataFrame(mlp_test_accs)\n",
        "df_gcn = pd.DataFrame(gcn_test_accs)\n",
        "# Reset index\n",
        "df_raw.reset_index(inplace=True)\n",
        "df_sgc.reset_index(inplace=True)\n",
        "df_asgc.reset_index(inplace=True)\n",
        "df_mlp.reset_index(inplace=True)\n",
        "df_gcn.reset_index(inplace=True)\n",
        "\n",
        "#melting dataframes and add method column\n",
        "df_raw = df_raw.melt(id_vars=['index'], var_name='dataset', value_name='Accuracy')\n",
        "df_raw['method'] = 'raw'\n",
        "df_sgc = df_sgc.melt(id_vars=['index'], var_name='dataset', value_name='Accuracy')\n",
        "df_sgc['method'] = 'sgc'\n",
        "df_asgc = df_asgc.melt(id_vars=['index'], var_name='dataset', value_name='Accuracy')\n",
        "df_asgc['method'] = 'asgc'\n",
        "df_mlp = df_mlp.melt(id_vars=['index'], var_name='dataset', value_name='Accuracy')\n",
        "df_mlp['method'] = 'mlp'\n",
        "df_gcn = df_gcn.melt(id_vars=['index'], var_name='dataset', value_name='Accuracy')\n",
        "df_gcn['method'] = 'gcn'\n",
        "\n",
        "#combining all dataframes\n",
        "df_final = pd.concat([df_raw, df_sgc, df_asgc, df_mlp, df_gcn], ignore_index=True)\n",
        "df_final.drop('index', axis=1, inplace=True)\n",
        "\n",
        "#getting unique datasets\n",
        "datasets = df_final['dataset'].unique()\n",
        "df_final['method'] = df_final['method'].str.upper()\n",
        "\n",
        "#creating a 5x2 grid of subplots\n",
        "fig, axs = plt.subplots(5, 2, figsize=(10, 20), sharey=True)\n",
        "\n",
        "# defining color palettes for the two different types of graphs\n",
        "palette1 = sns.color_palette(\"Blues\")\n",
        "palette2 = sns.color_palette(\"Greens\")\n",
        "\n",
        "#creating the barplot for the accuracy\n",
        "for i, dataset in enumerate(datasets):\n",
        "\n",
        "    ax = axs[i % 5, i // 5]\n",
        "    data = df_final[df_final['dataset'] == dataset]\n",
        "    palette = palette1 if i < 5 else palette2\n",
        "    sns.barplot(x='method', y='Accuracy', data=data, ax=ax, palette=palette)\n",
        "    ax.set_title(dataset)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./acc_barplot.png\")\n",
        "#files.download(\"./acc_barplot.png\") \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD3biqh4BLed"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(7, 6), sharex=True)\n",
        "\n",
        "#defining method groups\n",
        "methods = df_final['method'].unique()\n",
        "df_final['type'] = df_final['dataset'].apply(lambda x: 'Homophilous' if x in homoph_dsets else 'Heterophilous')\n",
        "\n",
        "#creating the boxplots\n",
        "sns.boxplot(x='method', y='Accuracy', data=df_final[df_final['type'] == 'Homophilous'], ax=axs[0], palette='Blues')\n",
        "sns.boxplot(x='method', y='Accuracy', data=df_final[df_final['type'] == 'Heterophilous'], ax=axs[1], palette='Greens')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_ylabel('Accuracy')\n",
        "axs[0].set_title('Homophilous')\n",
        "axs[1].set_title('Heterophilous')\n",
        "\n",
        "plt.xticks(range(len(methods)), methods)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./acc_boxplot.png\")\n",
        "#files.download(\"./acc_boxplot.png\") \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h4lH7XE1xJt"
      },
      "outputs": [],
      "source": [
        "#we then do the same process for the time dicts\n",
        "df_raw_time = pd.DataFrame(raw_tt_time)\n",
        "df_sgc_time = pd.DataFrame(sgc_tt_time)\n",
        "df_asgc_time = pd.DataFrame(asgc_tt_time)\n",
        "df_mlp_time = pd.DataFrame(mlp_tt_time)\n",
        "df_gcn_time = pd.DataFrame(gcn_tt_time)\n",
        "\n",
        "df_raw_time.reset_index(inplace=True)\n",
        "df_sgc_time.reset_index(inplace=True)\n",
        "df_asgc_time.reset_index(inplace=True)\n",
        "df_mlp_time.reset_index(inplace=True)\n",
        "df_gcn_time.reset_index(inplace=True)\n",
        "\n",
        "df_raw_time = df_raw_time.melt(id_vars=['index'], var_name='dataset', value_name='Time')\n",
        "df_raw_time['method'] = 'raw'\n",
        "df_sgc_time = df_sgc_time.melt(id_vars=['index'], var_name='dataset', value_name='Time')\n",
        "df_sgc_time['method'] = 'sgc'\n",
        "df_asgc_time = df_asgc_time.melt(id_vars=['index'], var_name='dataset', value_name='Time')\n",
        "df_asgc_time['method'] = 'asgc'\n",
        "df_mlp_time = df_mlp_time.melt(id_vars=['index'], var_name='dataset', value_name='Time')\n",
        "df_mlp_time['method'] = 'mlp'\n",
        "df_gcn_time = df_gcn_time.melt(id_vars=['index'], var_name='dataset', value_name='Time')\n",
        "df_gcn_time['method'] = 'gcn'\n",
        "\n",
        "df_final_time = pd.concat([df_raw_time, df_sgc_time, df_asgc_time, df_mlp_time, df_gcn_time], ignore_index=True)\n",
        "df_final_time.drop('index', axis=1, inplace=True)\n",
        "\n",
        "df_final_time['method'] = df_final_time['method'].str.upper()\n",
        "\n",
        "fig, axs = plt.subplots(5, 2, figsize=(10, 20), sharey=False)\n",
        "\n",
        "palette1 = sns.color_palette(\"Oranges\")\n",
        "palette2 = sns.color_palette(\"Purples\")\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "\n",
        "    ax = axs[i % 5, i // 5]\n",
        "    data = df_final_time[df_final_time['dataset'] == dataset]\n",
        "    palette = palette1 if i < 5 else palette2\n",
        "    sns.barplot(x='method', y='Time', data=data, ax=ax, palette=palette)\n",
        "    ax.set_title(dataset)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./time_barplot.png\")\n",
        "#files.download(\"./time_barplot.png\") \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XeL3x1q1znv"
      },
      "outputs": [],
      "source": [
        "#we then do the same process for the time dicts\n",
        "fig, axs = plt.subplots(2, 1, figsize=(7, 6), sharex=True)\n",
        "\n",
        "methods = df_final['method'].unique()\n",
        "df_final_time['type'] = df_final_time['dataset'].apply(\n",
        "    lambda x: 'Homophilous' if x in homoph_dsets else 'Heterophilous')\n",
        "\n",
        "\n",
        "sns.boxplot(x='method', y='Time', data=df_final_time[df_final_time['type'] == 'Homophilous'], ax=axs[0],\n",
        "            palette='Oranges')\n",
        "sns.boxplot(x='method', y='Time', data=df_final_time[df_final_time['type'] == 'Heterophilous'], ax=axs[1],\n",
        "            palette='Purples')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_ylabel('Time')\n",
        "axs[0].set_title('Homophilous')\n",
        "axs[1].set_title('Heterophilous')\n",
        "\n",
        "plt.xticks(range(len(methods)), methods)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./time_boxplot.png\")\n",
        "#files.download(\"./time_boxplot.png\") \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSnR-bFhBH0X"
      },
      "outputs": [],
      "source": [
        "df_final.to_csv('./df_final_acc.csv')\n",
        "df_final_time.to_csv('./df_final_time.csv')\n",
        "\n",
        "mean_values = df_final.groupby('method')['Accuracy'].mean()\n",
        "print(mean_values)\n",
        "std_values = df_final.groupby('method')['Accuracy'].std()\n",
        "print(std_values)\n",
        "\n",
        "print('-----------------------')\n",
        "\n",
        "mean_values = df_final_time.groupby('method')['Time'].mean()\n",
        "print(mean_values)\n",
        "\n",
        "std_values = df_final_time.groupby('method')['Time'].std()\n",
        "print(std_values)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}